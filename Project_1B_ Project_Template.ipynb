{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling with Cassandra\n",
    "## 1. Application Design\n",
    "A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.\n",
    "\n",
    "The aim is to create a Cassandra database with tables designed to optimize queries on song play analysis.\n",
    "\n",
    "## 2. Conceptual Data Model\n",
    "<img src=\"./images/01_conceptual_data_model.gif\" width=\"1000\"/>\n",
    "\n",
    "## 3. Defining Application Queries\n",
    "\n",
    "Create a non-sql data models to answer following questions for music streaming up Sparkify:\n",
    "* Q1: Look up artist, song title and length by `sessionId = 338`, and `itemInSession  = 4`\n",
    "* Q2: Look up artist, song title, user's first and last name by `userid = 10`, `sessionid = 182`\n",
    "* Q3: Look up all users (first and last name) who listened to particular song ex. `'All Hands Against His Own'`\n",
    "\n",
    "<img src=\"./images/02_application_queries.gif\" width=\"1000\"/>\n",
    "\n",
    "The Sparkify analysts working flow would be first to look to song popularity (Q3). Then select one of the returned users and check his/her songplay history (Q2) and deep dive to particular session (Q1).\n",
    "\n",
    "## 4. Logical Data Modeling\n",
    "\n",
    "\n",
    "## 2. Preprocessing log files\n",
    "Log files are consolidated to one file for easy extraction and loading to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Import Python packages\n",
    "import cassandra.cluster\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "from cassandra.cluster import Cluster\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821 new records copied into all_event_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Process log files\n",
    "def concatenate_files (read_path_pattern: [str, os.PathLike], write_path: [str, os.PathLike]) -> None:\n",
    "    \"\"\"Concatenate files matching read_path_pattern according the rules of UNIX shell\n",
    "\n",
    "    Parameters:\n",
    "        read_path_pattern: string or os.Pathlike\n",
    "            Path pattern to concatenate files ex: files/director/*.csv\n",
    "\n",
    "        write_path: string or os.Pathlike\n",
    "            Path to concatenated file\n",
    "    \"\"\"\n",
    "\n",
    "    # Delete existing file\n",
    "    if os.path.exists(write_path):\n",
    "        os.remove(write_path)\n",
    "\n",
    "    # Match files path according read_path_pattern\n",
    "    read_path_list = glob.glob(pathname=read_path_pattern, recursive=True)\n",
    "\n",
    "    # Write file extension\n",
    "    write_ext = os.path.splitext(write_path)[-1].lower()\n",
    "\n",
    "    # Read files and concatenate them using pandas dataframe\n",
    "    for read_path in read_path_list:\n",
    "\n",
    "        # Read file extension\n",
    "        read_ext = os.path.splitext(read_path)[-1].lower()\n",
    "\n",
    "        # Handle csv files\n",
    "        if (read_ext == '.csv') and (write_ext =='.csv'):\n",
    "\n",
    "            # Handle column heading\n",
    "            if os.path.exists(write_path):\n",
    "                header = False\n",
    "            else:\n",
    "                header = True\n",
    "\n",
    "            # Read and write csv file\n",
    "            df = pd.read_csv(read_path)\n",
    "\n",
    "            # Skip rows with empty artist\n",
    "            df = df.dropna(subset=['artist'])\n",
    "\n",
    "            df.to_csv(write_path, mode='a', header=header, index=False)\n",
    "\n",
    "    return None\n",
    "\n",
    "# Concatenate csv data files\n",
    "write_path = 'all_event_data.csv'\n",
    "read_path_pattern = os.path.join(os.getcwd(), 'event_data', '*.csv')\n",
    "concatenate_files(read_path_pattern, write_path)\n",
    "\n",
    "# Sum check\n",
    "with open(write_path, 'r', encoding = 'utf8') as f:\n",
    "    num_records = sum(1 for line in f)\n",
    "    print(f'{num_records} new records copied into {write_path}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "   sessionId  itemInSession  userId             artist  \\\n0        139              1       8            Des'ree   \n1        139              3       8            Mr Oizo   \n2        139              4       8         Tamba Trio   \n3        139              5       8     The Mars Volta   \n4        139              6       8  Infected Mushroom   \n\n                           song firstName lastName     length  \n0                  You Gotta Be    Kaylee  Summers  246.30812  \n1                       Flat 55    Kaylee  Summers  144.03873  \n2  Quem Quiser Encontrar O Amor    Kaylee  Summers  177.18812  \n3                     Eriatarka    Kaylee  Summers  380.42077  \n4               Becoming Insane    Kaylee  Summers  440.26730  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sessionId</th>\n      <th>itemInSession</th>\n      <th>userId</th>\n      <th>artist</th>\n      <th>song</th>\n      <th>firstName</th>\n      <th>lastName</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>139</td>\n      <td>1</td>\n      <td>8</td>\n      <td>Des'ree</td>\n      <td>You Gotta Be</td>\n      <td>Kaylee</td>\n      <td>Summers</td>\n      <td>246.30812</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>139</td>\n      <td>3</td>\n      <td>8</td>\n      <td>Mr Oizo</td>\n      <td>Flat 55</td>\n      <td>Kaylee</td>\n      <td>Summers</td>\n      <td>144.03873</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>139</td>\n      <td>4</td>\n      <td>8</td>\n      <td>Tamba Trio</td>\n      <td>Quem Quiser Encontrar O Amor</td>\n      <td>Kaylee</td>\n      <td>Summers</td>\n      <td>177.18812</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>139</td>\n      <td>5</td>\n      <td>8</td>\n      <td>The Mars Volta</td>\n      <td>Eriatarka</td>\n      <td>Kaylee</td>\n      <td>Summers</td>\n      <td>380.42077</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>139</td>\n      <td>6</td>\n      <td>8</td>\n      <td>Infected Mushroom</td>\n      <td>Becoming Insane</td>\n      <td>Kaylee</td>\n      <td>Summers</td>\n      <td>440.26730</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review consolidated file\n",
    "columns = {\n",
    "    'sessionId': int,\n",
    "    'itemInSession': int,\n",
    "    'userId': int,\n",
    "    'artist': str,\n",
    "    'song': str,\n",
    "    'firstName': str,\n",
    "    'lastName': str,\n",
    "    'length': float\n",
    "}\n",
    "df = pd.read_csv(write_path, usecols=columns.keys(), dtype=columns)[columns.keys()]\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up Apache Cassandra\n",
    "The cassandra cluster is created on local host with __simple replication strategy__ using 1 replica for purpose of development. For production purpose one can increase replication factor to ensure high availability using one datacenter. __Network topology strategy__ can be used when using more datacenters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Cassandra result set as pandas dataframe\n",
    "def pandas_factory(columns, rows):\n",
    "    return pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "# Set Cassandra cluster at local host\n",
    "cluster = Cluster()\n",
    "\n",
    "# Connect to cluster to execute queries\n",
    "session = cluster.connect()\n",
    "session.row_factory = pandas_factory\n",
    "session.default_fetch_size = None\n",
    "\n",
    "# Create a Keyspace\n",
    "query = '''\n",
    "    CREATE KEYSPACE IF NOT EXISTS sparkify\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "'''\n",
    "session.execute(query)\n",
    "\n",
    "# Set keyspace\n",
    "session.set_keyspace('sparkify')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Modeling Tasks\n",
    "### 4.1 Partition Key Rules\n",
    "It is important to understand the way how the Cassandra write and read to optimize performance. The [rules of thumb](https://medium.com/@herryg91/in-depth-why-modeling-in-cassandra-is-important-ba25081dafd5) are:\n",
    "1. Partition key should have (10 - 100)K rows\n",
    "2. Partition key should dhave < 100MB size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "def insert_df_into_cassandra (df:pd.DataFrame, session:cassandra.cluster.Session, query:str) -> None:\n",
    "    \"\"\"Extracts csv file into pandas dataframe and inserts into cassandra table.\n",
    "\n",
    "    Parameters:\n",
    "        df: pd.DataFrame\n",
    "            pandas dataframe to be inserted into cassandra\n",
    "\n",
    "        session: cassandra.cluster.Session\n",
    "            Connection to Cassandra cluster\n",
    "\n",
    "        query: string\n",
    "            Insert statement according CQL syntax rules\n",
    "    \"\"\"\n",
    "\n",
    "    # Load dataframe to cassandra table\n",
    "    prepared = session.prepare(query)\n",
    "    for _, series in df.iterrows():\n",
    "        values = tuple(series.to_list())\n",
    "        bound = prepared.bind(values)\n",
    "        session.execute(bound)\n",
    "\n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "# Drop tables\n",
    "tables = [\n",
    "    'song_plays_by_session',\n",
    "    'song_plays_by_user_session',\n",
    "    'users_by_song_play'\n",
    "]\n",
    "for table in tables:\n",
    "    session.execute(f'''\n",
    "        DROP TABLE IF EXISTS {table}\n",
    "    ''')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Song Plays by Session\n",
    "Give me the artist, song title and song's length in the music app history that was heard during  `sessionId = 338`, and `itemInSession  = 4`\n",
    "\n",
    "## 4.2.1 Primary Key Discussion\n",
    "The songplay record is uniquely identified by combination of `sessionId` and `itemInSession`. The 100K records with average rows size of 32B would result in average partition size of ~ 3.2MB. `sessionId` is not optimal for partitioning due to high cardinality (avg 9 rows per `sessionId`). The solution will be to use `shard_id`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does combination of ['sessionId', 'itemInSession'] crates unique row identifier?: True\n"
     ]
    }
   ],
   "source": [
    "# Combination of sessionId and itemInSession is unique identifier of the songplay\n",
    "primary_key_columns = ['sessionId', 'itemInSession']\n",
    "is_primary_key = (df.groupby(primary_key_columns).size() == 1).all()\n",
    "print(f'Does combination of {primary_key_columns} crates unique row identifier?: {is_primary_key}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average row size: 32B\n",
      "Median rows per sessionId partition: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Decide on partition key\n",
    "def average_row_size (df:pd.DataFrame) -> float:\n",
    "    return df.memory_usage(index=False).sum() / df.index.size\n",
    "\n",
    "def median_num_rows_per_partition(sr: pd.Series) -> [int, pd.Series]:\n",
    "    return sr.value_counts().median()\n",
    "\n",
    "partition_key = 'sessionId'\n",
    "columns = {\n",
    "    'sessionId': int,\n",
    "    'itemInSession': int,\n",
    "    'artist': str,\n",
    "    'song': str,\n",
    "    'length': float\n",
    "}\n",
    "\n",
    "avg_row_size = average_row_size(df[columns.keys()])\n",
    "print(f'Average row size: {avg_row_size:.0f}B')\n",
    "\n",
    "md_num_rows_per_partition = median_num_rows_per_partition(df[partition_key])\n",
    "print(f'Median rows per {partition_key} partition: {md_num_rows_per_partition}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rows per sessionId: 8.788659793814434\n",
      "Median rows per partition for shard_id: 6820.0\n",
      "Estimated maximum sessions per partition: 11378.299120234604\n"
     ]
    }
   ],
   "source": [
    "# Create shard partition key\n",
    "avg_items_per_session = df.groupby([partition_key]).size().mean()\n",
    "max_sessions_per_partition = 100_000 / avg_items_per_session\n",
    "df['shardId'] = (df[partition_key] // max_sessions_per_partition).astype('int')\n",
    "md_num_rows_per_partition = median_num_rows_per_partition(df['shardId'])\n",
    "print(f'Average rows per {partition_key}: {avg_items_per_session}')\n",
    "print(f'Median rows per partition for shard_id: {md_num_rows_per_partition}')\n",
    "print(f'Estimated maximum sessions per partition: {max_sessions_per_partition}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "# Fix maximum session per partition\n",
    "max_sessions_per_partition = 11378"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "data": {
      "text/plain": "<cassandra.cluster.ResultSet at 0x273a70c3760>"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table\n",
    "song_plays_by_session = '''\n",
    "    CREATE TABLE IF NOT EXISTS song_plays_by_session\n",
    "    (\n",
    "        shard_id INT,\n",
    "        session_id INT,\n",
    "        item_in_session INT,\n",
    "        artist_name TEXT,\n",
    "        song_title TEXT,\n",
    "        song_length DECIMAL,\n",
    "        PRIMARY KEY (shard_id, session_id, item_in_session)\n",
    "    )\n",
    "'''\n",
    "session.execute(song_plays_by_session)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Insert data into table\n",
    "columns = [\n",
    "    'shardId',\n",
    "    'sessionId',\n",
    "    'itemInSession',\n",
    "    'artist',\n",
    "    'song',\n",
    "    'length'\n",
    "]\n",
    "query = '''\n",
    "            INSERT INTO song_plays_by_session\n",
    "            (\n",
    "                shard_id,\n",
    "                session_id,\n",
    "                item_in_session,\n",
    "                artist_name,\n",
    "                song_title,\n",
    "                song_length\n",
    "            )\n",
    "            VALUES\n",
    "            (?, ?, ?, ?, ?, ?);\n",
    "    '''\n",
    "\n",
    "insert_df_into_cassandra(df[columns], session, query)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  artist_name                       song_title  \\\n0   Faithless  Music Matters (Mark Knight Dub)   \n\n                                        song_length  \n0  495.30729999999999790816218592226505279541015625  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist_name</th>\n      <th>song_title</th>\n      <th>song_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Faithless</td>\n      <td>Music Matters (Mark Knight Dub)</td>\n      <td>495.30729999999999790816218592226505279541015625</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify select statement\n",
    "session_id = 338\n",
    "item_in_session = 4\n",
    "shard_id = int(session_id // max_sessions_per_partition)\n",
    "query = f'''\n",
    "    SELECT\n",
    "        artist_name,\n",
    "        song_title,\n",
    "        song_length\n",
    "    FROM song_plays_by_session\n",
    "    WHERE\n",
    "        shard_id = {shard_id} AND session_id = {session_id} AND item_in_session = {item_in_session};\n",
    "'''\n",
    "result = session.execute(query)\n",
    "result._current_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 Song Plays by User by Session\n",
    "Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for `userid = 10`, `sessionid = 182`\n",
    "\n",
    "## 4.3.1 Primary Key Discussion\n",
    "The songplay record is uniquely identified by combination of `sessionId` and `itemInSession`. The 100K records with average rows size of 44B would result in average partition size of ~ 4.4MB. `userId` is not optimal for partitioning due to high cardinality (avg 15 rows per `userId`). The solution will be to use `shard_id` as a partition key and `user_id`, `session_id` as clustering columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average row size: 44B\n",
      "Median rows per userId partition: 15.0\n"
     ]
    }
   ],
   "source": [
    "# Decide on partition key\n",
    "partition_key = 'userId'\n",
    "columns = {\n",
    "    'userId': int,\n",
    "    'sessionId': int,\n",
    "    'itemInSession': int,\n",
    "    'artist': str,\n",
    "    'song': str,\n",
    "    'firstName': str,\n",
    "    'lastName': str\n",
    "}\n",
    "\n",
    "avg_row_size = average_row_size(df[columns.keys()])\n",
    "print(f'Average row size: {avg_row_size:.0f}B')\n",
    "\n",
    "md_num_rows_per_partition = median_num_rows_per_partition(df[partition_key])\n",
    "print(f'Median rows per {partition_key} partition: {md_num_rows_per_partition}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rows per userId: 71.04166666666667\n",
      "Median rows per partition for shard_id: 6820.0\n",
      "Estimated maximum users per partition: 1407.624633431085\n"
     ]
    }
   ],
   "source": [
    "# Create shard partition key\n",
    "avg_items_per_session = df.groupby([partition_key]).size().mean()\n",
    "max_users_per_partition = 100_000 / avg_items_per_session\n",
    "df['shardId'] = (df[partition_key] // max_users_per_partition).astype('int')\n",
    "md_num_rows_per_partition = median_num_rows_per_partition(df['shardId'])\n",
    "print(f'Average rows per {partition_key}: {avg_items_per_session}')\n",
    "print(f'Median rows per partition for shard_id: {md_num_rows_per_partition}')\n",
    "print(f'Estimated maximum users per partition: {max_users_per_partition}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "# Fix maximum users per partition\n",
    "max_users_per_partition = 1408"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "data": {
      "text/plain": "<cassandra.cluster.ResultSet at 0x273a8973850>"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table\n",
    "song_plays_by_user_session = '''\n",
    "    CREATE TABLE IF NOT EXISTS song_plays_by_user_session\n",
    "    (\n",
    "        shard_id INT,\n",
    "        user_id INT,\n",
    "        session_id INT,\n",
    "        item_in_session INT,\n",
    "        artist_name TEXT,\n",
    "        song_title TEXT,\n",
    "        user_first_name TEXT,\n",
    "        user_last_name TEXT,\n",
    "        PRIMARY KEY (shard_id, user_id, session_id, item_in_session)\n",
    "    )\n",
    "    WITH CLUSTERING ORDER BY (user_id ASC, session_id ASC, item_in_session ASC);\n",
    "'''\n",
    "session.execute(song_plays_by_user_session)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Insert data into table\n",
    "columns = [\n",
    "    'shardId',\n",
    "    'userId',\n",
    "    'sessionId',\n",
    "    'itemInSession',\n",
    "    'artist',\n",
    "    'song',\n",
    "    'firstName',\n",
    "    'lastName'\n",
    "]\n",
    "query = '''\n",
    "            INSERT INTO song_plays_by_user_session\n",
    "            (\n",
    "                shard_id,\n",
    "                user_id,\n",
    "                session_id,\n",
    "                item_in_session,\n",
    "                artist_name,\n",
    "                song_title,\n",
    "                user_first_name,\n",
    "                user_last_name\n",
    "            )\n",
    "            VALUES\n",
    "            (?, ?, ?, ?, ?, ?, ?, ?);\n",
    "    '''\n",
    "\n",
    "insert_df_into_cassandra(df[columns], session, query)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "data": {
      "text/plain": "         artist_name                                         song_title  \\\n0   Down To The Bone                                 Keep On Keepin' On   \n1       Three Drives                                        Greece 2000   \n2  Sebastien Tellier                                          Kilometer   \n3      Lonnie Gordon  Catch You Baby (Steve Pitron & Max Sanna Radio...   \n\n  user_first_name user_last_name  \n0          Sylvie           Cruz  \n1          Sylvie           Cruz  \n2          Sylvie           Cruz  \n3          Sylvie           Cruz  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist_name</th>\n      <th>song_title</th>\n      <th>user_first_name</th>\n      <th>user_last_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Down To The Bone</td>\n      <td>Keep On Keepin' On</td>\n      <td>Sylvie</td>\n      <td>Cruz</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Three Drives</td>\n      <td>Greece 2000</td>\n      <td>Sylvie</td>\n      <td>Cruz</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sebastien Tellier</td>\n      <td>Kilometer</td>\n      <td>Sylvie</td>\n      <td>Cruz</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Lonnie Gordon</td>\n      <td>Catch You Baby (Steve Pitron &amp; Max Sanna Radio...</td>\n      <td>Sylvie</td>\n      <td>Cruz</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify select statement\n",
    "# ame of artist, song (sorted by itemInSession) and user (first and last name) for `userid = 10`, `sessionid = 182`\n",
    "user_id = 10\n",
    "session_id = 182\n",
    "shard_id = int(user_id // max_users_per_partition)\n",
    "\n",
    "query = f'''\n",
    "    SELECT\n",
    "        artist_name,\n",
    "        song_title,\n",
    "        user_first_name,\n",
    "        user_last_name\n",
    "    FROM song_plays_by_user_session\n",
    "    WHERE\n",
    "        shard_id = {shard_id} AND user_id = {user_id} AND session_id = {session_id};\n",
    "'''\n",
    "result = session.execute(query)\n",
    "result._current_rows"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 Users by Song Play\n",
    "Give me every user name (first and last) in my music app history who listened to the song `All Hands Against His Own`\n",
    "\n",
    "## 4.4.1 Primary Key Discussion\n",
    "The songplay record is uniquely identified by combination of `sessionId` and `itemInSession`. To get the list of users listening to particular song the combination of `song` and `user_id` is sufficient as primary key. The 100K records with average rows size of 28B would result in average partition size of ~ 2.8MB. `song` is not optimal for partitioning due to high cardinality (avg 15 rows per `userId` and avg 1 rows per `song`). The solution will be to use `shard_id` as a partition key and `user_id`, `song` as clustering columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average row size: 28B\n",
      "Median rows per song partition: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Decide on partition key\n",
    "partition_key = 'song'\n",
    "columns = {\n",
    "    'userId': int,\n",
    "    'song': str,\n",
    "    'firstName': str,\n",
    "    'lastName': str\n",
    "}\n",
    "\n",
    "avg_row_size = average_row_size(df[columns.keys()])\n",
    "print(f'Average row size: {avg_row_size:.0f}B')\n",
    "\n",
    "md_num_rows_per_partition = median_num_rows_per_partition(df[partition_key])\n",
    "print(f'Median rows per {partition_key} partition: {md_num_rows_per_partition}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for //: 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\envs\\DEND\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001B[0m in \u001B[0;36m_na_arithmetic_op\u001B[1;34m(left, right, op, is_cmp)\u001B[0m\n\u001B[0;32m    165\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 166\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mleft\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mright\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    167\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DEND\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001B[0m in \u001B[0;36mevaluate\u001B[1;34m(op, a, b, use_numexpr)\u001B[0m\n\u001B[0;32m    239\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0m_evaluate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mop\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mop_str\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[misc]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 240\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_evaluate_standard\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mop\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mop_str\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    241\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DEND\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001B[0m in \u001B[0;36m_evaluate_standard\u001B[1;34m(op, op_str, a, b)\u001B[0m\n\u001B[0;32m     68\u001B[0m         \u001B[0m_store_test_result\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     70\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: unsupported operand type(s) for //: 'str' and 'float'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_7016/1992329195.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mavg_items_per_session\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpartition_key\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mmax_users_per_partition\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m100_000\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mavg_items_per_session\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'shardId'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpartition_key\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m//\u001B[0m \u001B[0mmax_users_per_partition\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'int'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[0mmd_num_rows_per_partition\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmedian_num_rows_per_partition\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'shardId'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'Average rows per {partition_key}: {avg_items_per_session}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DEND\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001B[0m in \u001B[0;36mnew_method\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m     67\u001B[0m         \u001B[0mother\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mitem_from_zerodim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mother\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mother\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     70\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mnew_method\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DEND\\lib\\site-packages\\pandas\\core\\arraylike.py\u001B[0m in \u001B[0;36m__floordiv__\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m    122\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0munpack_zerodim_and_defer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"__floordiv__\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    123\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__floordiv__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mother\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 124\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_arith_method\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mother\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moperator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloordiv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    125\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    126\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0munpack_zerodim_and_defer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"__rfloordiv\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DEND\\lib\\site-packages\\pandas\\core\\series.py\u001B[0m in \u001B[0;36m_arith_method\u001B[1;34m(self, other, op)\u001B[0m\n\u001B[0;32m   5524\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5525\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merrstate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mall\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"ignore\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 5526\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marithmetic_op\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlvalues\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrvalues\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   5527\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5528\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_construct_result\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mres_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DEND\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001B[0m in \u001B[0;36marithmetic_op\u001B[1;34m(left, right, op)\u001B[0m\n\u001B[0;32m    222\u001B[0m         \u001B[0m_bool_arith_check\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mop\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mleft\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mright\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 224\u001B[1;33m         \u001B[0mres_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_na_arithmetic_op\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mleft\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mright\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    225\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    226\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mres_values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DEND\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001B[0m in \u001B[0;36m_na_arithmetic_op\u001B[1;34m(left, right, op, is_cmp)\u001B[0m\n\u001B[0;32m    171\u001B[0m             \u001B[1;31m# Don't do this for comparisons, as that will handle complex numbers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    172\u001B[0m             \u001B[1;31m#  incorrectly, see GH#32047\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 173\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_masked_arith_op\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mleft\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mright\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    174\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    175\u001B[0m             \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DEND\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001B[0m in \u001B[0;36m_masked_arith_op\u001B[1;34m(x, y, op)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    130\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmask\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0many\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 131\u001B[1;33m             \u001B[0mresult\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mxrav\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    132\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    133\u001B[0m     \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mputmask\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m~\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnan\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: unsupported operand type(s) for //: 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "# Create shard partition key\n",
    "avg_items_per_session = df.groupby([partition_key]).size().mean()\n",
    "max_users_per_partition = 100_000 / avg_items_per_session\n",
    "df['shardId'] = (df[partition_key] // max_users_per_partition).astype('int')\n",
    "md_num_rows_per_partition = median_num_rows_per_partition(df['shardId'])\n",
    "print(f'Average rows per {partition_key}: {avg_items_per_session}')\n",
    "print(f'Median rows per partition for shard_id: {md_num_rows_per_partition}')\n",
    "print(f'Estimated maximum users per partition: {max_users_per_partition}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "# Fix maximum users per partition\n",
    "max_users_per_partition = 1408"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "data": {
      "text/plain": "<cassandra.cluster.ResultSet at 0x273a70d82e0>"
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table\n",
    "users_by_song_play = '''\n",
    "    CREATE TABLE IF NOT EXISTS users_by_song_play\n",
    "    (\n",
    "        shard_id INT,\n",
    "        user_id INT,\n",
    "        song_title INT,\n",
    "        user_first_name TEXT,\n",
    "        user_last_name TEXT,\n",
    "        PRIMARY KEY (shard_id, user_id, song_title)\n",
    "    )\n",
    "'''\n",
    "session.execute(users_by_song_play)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Insert data into table\n",
    "columns = [\n",
    "    'shardId',\n",
    "    'userId',\n",
    "    'song',\n",
    "    'firstName',\n",
    "    'lastName'\n",
    "]\n",
    "query = '''\n",
    "            INSERT INTO users_by_song_play\n",
    "            (\n",
    "                shard_id,\n",
    "                user_id,\n",
    "                song_title,\n",
    "                user_first_name,\n",
    "                user_last_name\n",
    "            )\n",
    "            VALUES\n",
    "            (?, ?, ?, ?, ?);\n",
    "    '''\n",
    "\n",
    "insert_df_into_cassandra(df[columns], session, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(user_id=29, user_first_name='Jacqueline', user_last_name='Lynch')\n",
      "Row(user_id=80, user_first_name='Tegan', user_last_name='Levine')\n",
      "Row(user_id=95, user_first_name='Sara', user_last_name='Johnson')\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "query = '''\n",
    "    SELECT\n",
    "        user_id,\n",
    "        user_first_name,\n",
    "        user_last_name\n",
    "    FROM users_by_song_play\n",
    "    WHERE\n",
    "        song_title = 'All Hands Against His Own'\n",
    "'''\n",
    "rows = session.execute(query)\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Drop the table before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}